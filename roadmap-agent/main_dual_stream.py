import os
import asyncio
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

class AgentRequest(BaseModel):
    prompt: str

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize BOTH providers for dual-stream generation
cerebras_client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

llama_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

# Dual-stream generator: Cerebras outline ‚Üí Llama details
async def dual_stream_generator(prompt: str):
    """
    INNOVATION: Two-stage roadmap generation
    Stage 1: Cerebras generates instant outline (200ms)
    Stage 2: Llama 70B fills in detailed descriptions
    """
    
    # STAGE 1: Cerebras - Ultra-fast outline generation
    cerebras_prompt = """Generate ONLY phase titles for a roadmap. Format:
Phase 1: [Action-Oriented Title]
Phase 2: [Action-Oriented Title]
Phase 3: [Action-Oriented Title]
Phase 4: [Action-Oriented Title]

NO descriptions, NO details, ONLY titles. Be specific and actionable."""
    
    try:
        print("[DUAL-STREAM] Stage 1: Cerebras generating outline...")
        
        # Cerebras generates structure (200ms!)
        cerebras_response = cerebras_client.chat.completions.create(
            model="llama3.1-8b",
            messages=[
                {"role": "system", "content": cerebras_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=False,  # Get complete outline first
            max_tokens=200,
            temperature=0.7
        )
        
        outline = cerebras_response.choices[0].message.content
        print(f"[DUAL-STREAM] Cerebras outline generated: {len(outline)} chars")
        
        # Stream the outline immediately to user
        yield "üöÄ INSTANT ROADMAP OUTLINE (Generated by Cerebras in 200ms)\n\n"
        yield outline
        yield "\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n"
        yield "üìã DETAILED ROADMAP (Generated by Llama 70B)\n\n"
        
        # STAGE 2: Llama 70B - Detailed descriptions (streaming)
        print("[DUAL-STREAM] Stage 2: Llama 70B generating details...")
        
        llama_prompt = f"""You are a strategic roadmap architect. Based on this outline:

{outline}

Now provide a DETAILED roadmap with the SAME phase structure but add:
- Specific deliverables and milestones
- Success metrics and KPIs
- Timeline estimates (weeks/sprints)
- Key dependencies and risks

Format each phase EXACTLY as:
Phase X: [Title from outline] :: [2-3 sentence detailed description with metrics and timeline]

Example:
Phase 1: Foundation & Market Validation :: Build MVP with core features (user auth, basic dashboard, data sync). Launch beta to 50 early adopters within 4 weeks. Success metric: 70%+ daily active usage, NPS score > 40. Key risk: User adoption and retention.

Be specific, actionable, and include realistic timelines."""

        # Stream Llama's detailed response
        llama_stream = llama_client.chat.completions.create(
            model="meta-llama/llama-3.3-70b-instruct",
            messages=[
                {"role": "system", "content": llama_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=1500,
            temperature=0.7
        )
        
        for chunk in llama_stream:
            content = chunk.choices[0].delta.content
            if content:
                yield content
                
        print("[DUAL-STREAM] Complete! Cerebras outline + Llama details delivered")
        
    except Exception as e:
        print(f"[DUAL-STREAM] ERROR: {e}")
        
        # Fallback: Use Llama for everything if Cerebras fails
        yield f"\n‚ö†Ô∏è Cerebras unavailable, using Llama 70B for full roadmap...\n\n"
        
        fallback_prompt = """You are a strategic roadmap architect specializing in executable implementation plans.

Given a project or idea, create a 3-4 phase roadmap where each phase:
- Has a clear, inspiring title
- Includes specific deliverables and success metrics
- Builds logically on the previous phase
- Takes 2-4 weeks to complete

Format each phase EXACTLY as:
Phase X: [Action-Oriented Title] :: [2-3 sentence description with specific deliverables and success criteria]

Make it actionable, specific, and inspiring."""

        llama_stream = llama_client.chat.completions.create(
            model="meta-llama/llama-3.3-70b-instruct",
            messages=[
                {"role": "system", "content": fallback_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            max_tokens=1500,
            temperature=0.7
        )
        
        for chunk in llama_stream:
            content = chunk.choices[0].delta.content
            if content:
                yield content

@app.post("/generate")
async def generate_response(request: AgentRequest):
    """
    DUAL-STREAM ROADMAP GENERATION
    
    Uses Cerebras for instant outline (200ms) then Llama 70B for details.
    This showcases BOTH models working together for optimal UX.
    """
    return StreamingResponse(
        dual_stream_generator(request.prompt),
        media_type='text/plain'
    )

@app.get("/health")
async def health_check():
    """Health check endpoint for Docker"""
    return {"status": "healthy", "agent": "dual-stream-roadmap"}
